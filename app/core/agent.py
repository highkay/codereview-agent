from typing import List, Dict, Any, Optional
from app.models.config import AppConfig
from app.services.llm_service import LLMService, CodeContext, ReviewResult, QualityMetrics
from app.core.scm import SCMProvider, CommitDiff, ReviewComment
from loguru import logger
import fnmatch
import os

class CodeReviewAgent:
    def __init__(self, config: AppConfig, scm: SCMProvider, llm: LLMService) -> None:
        self.config = config
        self.scm = scm
        self.llm = llm
        logger.info("CodeReviewAgent initialized with config: {}", config)

    def _filter_files(self, files: List[dict]) -> List[dict]:
        """è¿‡æ»¤ä¸éœ€è¦è¯„å®¡çš„æ–‡ä»¶"""
        filtered = []
        for file in files:
            should_ignore = any(
                fnmatch.fnmatch(file["filename"], pattern)
                for pattern in self.config.review.ignore_patterns
            )
            if not should_ignore:
                filtered.append(file)
        return filtered

    async def _collect_context(self, owner: str, repo: str, commit_diff: CommitDiff) -> Optional[CodeContext]:
        # è¿‡æ»¤æ–‡ä»¶
        filtered_files = self._filter_files(commit_diff.files)
        if not filtered_files:
            logger.info("No files to review after filtering for commit {}", commit_diff.commit_id[:8])
            return None
            
        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„ä¸Šä¸‹æ–‡
        files_context = []
        window_size = self.config.scm.context_window
        
        for file in filtered_files:
            if "filename" not in file:
                logger.error("Invalid file data - missing filename")
                continue
                
            file_path = file["filename"]
            file_type = os.path.splitext(file_path)[1][1:] if os.path.splitext(file_path)[1] else "unknown"
            
            try:
                # è·å–æ–‡ä»¶ä¸Šä¸‹æ–‡
                context = await self.scm.get_file_context(
                    owner,
                    repo,
                    file_path,
                    commit_diff.commit_id,
                    1,
                    window_size * 2
                )
                
                if not context:
                    logger.warning("No context returned for file: {}", file_path)
                    continue
                    
                files_context.append({
                    "file_path": file_path,
                    "file_type": file_type,
                    "context": context
                })
            except Exception as e:
                logger.error("Error getting context for file {}: {}", file_path, str(e))
                continue
                
        if not files_context:
            logger.warning("No valid file contexts collected for commit {}", commit_diff.commit_id[:8])
            return None
            
        # åˆ›å»ºå¹¶éªŒè¯ä¸Šä¸‹æ–‡å¯¹è±¡
        try:
            context = CodeContext(
                diff=commit_diff.diff_content,
                files_context=files_context,
                metadata={
                    "commit_id": commit_diff.commit_id,
                    "commit_message": commit_diff.commit_message
                }
            )
            return context
        except Exception as e:
            logger.error("Error creating CodeContext: {}", str(e))
            return None

    async def _analyze_code(self, context: CodeContext) -> ReviewResult:
        """åˆ†ææ•´ä¸ªcommitçš„ä»£ç å˜æ›´"""
        logger.debug("Analyzing commit: {} - {}", 
                    context.metadata["commit_id"][:8],
                    context.metadata["commit_message"])
        
        try:
            result = await self.llm.analyze_code(context)
            
            # è®°å½•è¯„å®¡ç»“æœ
            logger.info("Code analysis completed for commit {} with scores:", 
                       context.metadata["commit_id"][:8])
            logger.info("- Overall Score: {}/10", result.score)
            logger.info("- Security: {}/10", result.quality_metrics.security_score)
            logger.info("- Performance: {}/10", result.quality_metrics.performance_score)
            logger.info("- Readability: {}/10", result.quality_metrics.readability_score)
            logger.info("- Best Practices: {}/10", result.quality_metrics.best_practice_score)
            
            if result.security_issues:
                logger.warning("Found {} security issues in commit {}", 
                             len(result.security_issues), 
                             context.metadata["commit_id"][:8])
            
            return result
        except Exception as e:
            logger.error("Error analyzing code: {}\nFull error: {}", str(e), repr(e))
            # è¿”å›ä¸€ä¸ªé»˜è®¤çš„è¯„å®¡ç»“æœ
            return ReviewResult(
                score=0,
                comments=["ä»£ç è¯„å®¡è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯"],
                suggestions=[],
                security_issues=[],
                quality_metrics=QualityMetrics(
                    security_score=0,
                    performance_score=0,
                    readability_score=0,
                    best_practice_score=0
                )
            )

    def _generate_comments(self, result: ReviewResult, context: CodeContext) -> List[ReviewComment]:
        """ç”Ÿæˆè¯„å®¡è¯„è®º"""
        logger.debug("Generating comments for commit: {}", context.metadata["commit_id"][:8])
        comments = []
        
        # æ·»åŠ æ€»ä½“è¯„åˆ†è¯„è®º
        overall_comment = [
            "# ğŸ” ä»£ç è¯„å®¡æŠ¥å‘Š",
            "",
            f"## ğŸ“Š è¯„åˆ†æ¦‚è§ˆ ({result.score:.1f}/10)",
            "",
            "| è¯„å®¡ç»´åº¦ | å¾—åˆ† | æƒé‡ |",
            "|---------|------|------|",
            f"| ğŸ›¡ï¸ å®‰å…¨æ€§ | {result.quality_metrics.security_score:.1f}/10 | 30% |",
            f"| âš¡ æ€§èƒ½ | {result.quality_metrics.performance_score:.1f}/10 | 20% |",
            f"| ğŸ“– å¯è¯»æ€§ | {result.quality_metrics.readability_score:.1f}/10 | 20% |",
            f"| âœ¨ æœ€ä½³å®è·µ | {result.quality_metrics.best_practice_score:.1f}/10 | 30% |",
            ""
        ]
        
        if result.issues:
            overall_comment.extend([
                "## ğŸ’¡ éœ€è¦æ”¹è¿›çš„åœ°æ–¹",
                ""
            ])
            for issue in result.issues:
                overall_comment.extend([
                    f"### {issue.file_path}",
                    f"- ä½ç½®ï¼šç¬¬{issue.start_line}è¡Œ" + (f"-{issue.end_line}è¡Œ" if issue.end_line else ""),
                    f"- é—®é¢˜ï¼š{issue.description}",
                    f"- å»ºè®®ï¼š{issue.suggestion}",
                    ""
                ])
        
        if result.security_issues:
            overall_comment.extend([
                "## âš ï¸ å®‰å…¨é—®é¢˜",
                ""
            ])
            for issue in result.security_issues:
                severity_icon = "ğŸ”´" if issue.severity.lower() == "high" else "ğŸŸ¡"
                overall_comment.extend([
                    f"### {severity_icon} {issue.file_path}",
                    f"- ä¸¥é‡ç¨‹åº¦ï¼š{issue.severity}",
                    f"- ä½ç½®ï¼šç¬¬{issue.start_line}è¡Œ" + (f"-{issue.end_line}è¡Œ" if issue.end_line else ""),
                    f"- é—®é¢˜ï¼š{issue.description}",
                    f"- å»ºè®®ï¼š{issue.suggestion}",
                    ""
                ])
        
        comments.append(ReviewComment(
            path=context.metadata["commit_message"],
            line=1,
            body="\n".join(overall_comment),
            commit_id=context.metadata["commit_id"]
        ))
        
        logger.info("Generated {} review comments", len(comments))
        return comments

    async def review_pr(self, owner: str, repo: str, pr_id: str) -> bool:
        """æ‰§è¡ŒPRè¯„å®¡çš„ä¸»æµç¨‹"""
        logger.info("Starting PR review for {}/{} #{}", owner, repo, pr_id)
        try:
            # è·å–PRçš„æ‰€æœ‰commitsåŠå…¶diff
            commit_diffs = await self.scm.get_diff(owner, repo, pr_id)
            logger.info("Found {} commits to review in PR", len(commit_diffs))
            
            all_results = []
            for commit_diff in commit_diffs:
                try:
                    logger.info("Reviewing commit: {} - {}", 
                              commit_diff.commit_id[:8], 
                              commit_diff.commit_message)
                    
                    # æ”¶é›†æ•´ä¸ªcommitçš„ä¸Šä¸‹æ–‡
                    context = await self._collect_context(owner, repo, commit_diff)
                    if not context:
                        logger.warning("Skipping commit {} due to no reviewable files", 
                                     commit_diff.commit_id[:8])
                        continue
                    
                    # åˆ†ææ•´ä¸ªcommitçš„ä»£ç 
                    result = await self._analyze_code(context)
                    all_results.append((result, context))
                    
                    # ç”Ÿæˆå¹¶å‘é€è¯„è®º
                    comments = self._generate_comments(result, context)
                    
                    await self.scm.post_comment(owner, repo, pr_id, comments)
                    logger.info("Posted review comments for commit {}", commit_diff.commit_id[:8])
                except Exception as commit_error:
                    logger.error("Error processing commit {}: {}\nFull error: {}", 
                               commit_diff.commit_id[:8], str(commit_error), repr(commit_error))
                    continue
            
            # å¤„ç†è¯„å®¡ç»“æœ - ä½¿ç”¨æœ€ä½åˆ†ä½œä¸ºæœ€ç»ˆåˆ†æ•°
            if all_results:
                min_score = min(r.score for r, _ in all_results)
                logger.info("PR review completed with minimum score: {}", min_score)
                if min_score >= self.config.review.quality_threshold:
                    logger.info("PR quality meets threshold ({}), attempting to approve and merge", 
                              self.config.review.quality_threshold)
                    # å…ˆæ‰¹å‡†PR
                    await self.scm.approve_pr(owner, repo, pr_id)
                    # å†åˆå¹¶PR
                    await self.scm.merge_pr(owner, repo, pr_id)
                else:
                    logger.info("PR quality below threshold ({} < {})", 
                              min_score, self.config.review.quality_threshold)
            
            return True
            
        except Exception as e:
            logger.error("Error reviewing PR {}: {}\nFull error: {}", pr_id, str(e), repr(e))
            return False 